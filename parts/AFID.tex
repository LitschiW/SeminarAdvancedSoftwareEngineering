

\subsection{Automated Feature Interaction Detection}\label{sec:AFID}

Automated feature interaction detection (\AFID) is a measurement-based approach to predicting the performance of a highly configurable system.
It was developed by \citet{AutomatedFeatureDetectionSiegmund2012}. The following section is also based on the proposing paper of \AFID~\cite{AutomatedFeatureDetectionSiegmund2012}. 
\noindent
Under the usage of linear regression \AFID~tries to determine a performance influence value for each feature and feature interaction. A \textit{feature interaction} is defined as an unexpected influence on the performance of a system when using a specific feature combination. Furthermore a goal of \citet{AutomatedFeatureDetectionSiegmund} was also to find explainable predictions.
In the conducted experiments of \citet{CostEfficientSampling_Gou_Siegmund_2015} and \citet{AutomatedFeatureDetectionSiegmund2012} this method reaches average accuracies of 85\% and 95\% respectively.
\\
\noindent
The general process of \AFID~is displayed in \cref{fig:AFIDGerneral}.
It can be divided into two different steps:
\begin{enumerate}
	\item Finding interacting Features.
	\item Measuring the performance influence of feature interactions.
\end{enumerate} 
\begin{figure}
	\centering
	\scalebox{.8}{
		\includesvg{figures/AFIDGeneral}
	}
	\caption{General steps of \AFID.}
	\label{fig:AFIDGerneral}
\end{figure}
For the first step some notation is needed. For simplicity \AFID~is defined for binary options only.
The composition of performance influencing units (features or feature interactions) is denoted by a $\cdot$. In case two features are used simultaneously it is denoted by using a $\times$. This would also be another way to describe a configuration. So a program $P$ that uses the two features $a$ and $b$ can be denoted as $P= a \times b$.\\
To get the performance of $P$, $\Pi(P)$ has to be calculated. The exact definition of the performance influence determining function $\Pi$ can be found in the paper of \citet{AutomatedFeatureDetectionSiegmund2012}. For now, it is important to note that when calculating $\Pi(P)=\Pi(a \times b)$ not only the performance influence of $a$ and $b$ are necessary, but also $a\#b$ has to be considered. The latter is the performance influence of the possible interaction between $a$ and $b$. 
This means $\Pi(a \times b) = \Pi(a\#b) + \Pi(a) + \Pi(b)$. Higher order interactions are also possible: When using a program configuration $P_2 = a \times b \times c$ then $\Pi(P_2) = \Pi(a) +  \Pi(b) +  \Pi(c) +  \Pi(a\#b) +  \Pi(a\#c) +  \Pi(b\#c) +  \Pi(a\#b\#c)$. Some interactions do not exist or have an influence on the system, those will be ignored by the algorithm. Otherwise, a \textit{brute-force} solution would be equally efficient.

Finding these interactions requires looking for all interacting features first. This is done in by intelligently measuring certain configurations. \AFID~defines a feature $a$ as interacting when
\begin{equation}\label{def:equation}
a \text{ interacts} \Leftrightarrow \exists C,D \subseteq \mathcal{C}| C \neq D  \land	 |\Delta a_C - \Delta a_D| \leq t
\end{equation}
with 
\begin{equation}
\begin{split}
\Delta a_C &= \Pi(C\times a) - \Pi(C)\\
&=\Pi(a\# C) + \Pi(a).
\end{split}
\end{equation}
$t$ is a threshold depending on the given performance metric.
Using these two equations it can be determined, whether a feature is interacting with other features using 4 measurements.
These include $\Delta a_{min} = \Pi(a \times min(a)) - \Pi(min(a))$ and $\Delta a_{max} = \Pi(a \times max(a)) - \Pi(max(a))$. $min(a)$ is a configuration, that contains the minimum possible features without using $a$. Simultaneously, $max(a)$ is also a configuration that contains the maximum amount of possible features without $a$. 
Once these measurements are done \cref{def:equation} can be applied with $C=\Delta a_{min}$ and $D=\Delta a_{max}$. This is done for all features to find the interacting ones.
If a feature $f$ is not found to be interactive, its performance influence $\Pi(f)$ equals $\Delta f_{min}$.

Once all interacting features are found, the search for the actual interactions starts. In this second step three different heuristics are used to determine which interactions are searched for.
 
 \newcommand{\oitem}[2]{{\item[{\parbox[t][0pt][t]{\leftmargin}{\raggedleft #1}}] {\parbox[t]{\textwidth-\leftmargin}{#2}}}}
 \begin{itemize}[leftmargin=4cm]
 	\setlength\itemsep{1em}
 	\oitem{Pair-Wise~Heuristic (PW):\label{lab:PW}}{ Most groups of interacting features appear in the size of two \cite{AutomatedFeatureDetectionSiegmund2012,AnalysisOfTheVariabilityInFortyPreprocessor_BasedSPLLiebig}. So it makes sense to look for pair interaction first.}
 	\oitem{Higher-Order Interactions Heuristic (HO):\label{lab:HO}}{
 		\citet{AutomatedFeatureDetectionSiegmund2012} only look at higher order interactions of the rank of three. Even higher ranks would take up too many measurement resources.
 	}
 	\oitem{Hot-Spot Features Heuristic(HS):\label{lab:HS}}{
 		Based on \cite{FeatureCohesioninSPL} and \cite{CanWeAvoidHighCoupling?} \citet{AutomatedFeatureDetectionSiegmund2012} assume that hot spot features exist. At last these specific type of interactions are findable too.
 	}
 \end{itemize}

\begin{figure}[t]
	\centering
	\includegraphics[page=6,clip,trim=3.3cm 21cm 12.7cm 2.8cm]{"Paper/Predicting performance via automated feature-interaction detection".pdf}
	\captionsetup{width=0.95\linewidth}
	\caption{Implication tree example found in \cite{AutomatedFeatureDetectionSiegmund2012}. }
	\label{fig:ImplicationTree}
\end{figure}



The three heuristics will be applied in the order of PW $\rightarrow$ HO $\rightarrow$ HS and use the data provided by the previous ones. Using a SAT-Solver an implication-graph as seen in \cref{fig:ImplicationTree} is generated. Each implication-chain in this tree should have at least one interacting feature. When analysing the tree each chain is walked from the top down.

First the influence of every feature on another chain is measured (\hyperref[lab:PW]{PW-heuristic}). In the example of \cref{fig:ImplicationTree} the interactions would be measured in this order: $F1\#F6$,$ F1\#F7$,$ F4\#F6$, $F4\#F7$, $F6\#F11$, $F7\#F11$, $F1\#F11$, $F4\#F11$. Once an interaction impact exceeds a threshold it is recorded.

Secondly, the \hyperref[lab:HO]{higher order interaction heuristic} can be applied. Seconded order interactions can be relatively easily found by looking hat the results of the PW-Heuristik. Three features that interact pair-wise are likely to interact in a third order interaction. For example, looking at features $a$, $b$ and $c$. If $\Delta a\#b_{C1}$ and $\Delta b\#c_{C2}$ have been recorded, $\{a\#b, b\#c, a\#c\}$ all have to be non-zero to find a third order interaction. Interactions with and order higher than three are not considered to prevent too many measurements.

Lastly, Hot-Spot features may be detected (\hyperref[lab:HS]{HS-heuristic}). This is done by counting the interactions per feature. If the number of interactions of a feature is above a certain threshold (e.g. the arithmetic mean) it is categorized as a Hot-Spot feature. Based on these hotspot features further third order interactions are explored. Again higher order interactions are not considered. \\
After applying the three heuristics all detected interacting features or feature combinations are assigned a $\Delta$ to represent their performance influence on the program.

\begin{wrapfigure}[11]{r}{.5\textwidth}
	\centering
	\vspace{-\baselineskip}
	\captionof{table}{Results of average accurcy found by \citet{AutomatedFeatureDetectionSiegmund2012}.}
	\label{tab:avgAccuracy}
	\begin{tabular}{c|c}
		Approach&avg. Accuracy\\\midrule[1pt]
		FW&79.7\%\\\hline
		PW&91\%\\\hline
		HO&93.7\%\\\hline
		HS&95.4\%\\\hline
	\end{tabular}
\end{wrapfigure}

As mentioned, \citet{AutomatedFeatureDetectionSiegmund2012} tested \AFID~on six different software systems. Each program was tested under four approaches: Feature-Wise, Pair-Wise, Higher-Order, Hot-Spot (in this order). Each approach also used the data found by the previous one. Accordingly, the results get better the more heuristics are used as \cref{tab:avgAccuracy} shows. Using only the FW-heuristic means, that interactions are not considered at all. However, the accuracy when using this heuristic is already at 80\% on average. A significant improvement can be made by using the PW heuristic. It uses  8.5 times more measurements than the FW-heuristic on average, but improves the accuracy to 91\%. Using the HO- or HS-heuristics improves the accuracy further by 2-4\%. However, for Apache, using the HO over the PW heuristic even deteriorated the average result by 3.9\% and doubled the standard deviation. As expected, using the HS-heuristic gives the best accuracy for all 6 tested applications. \citet{AutomatedFeatureDetectionSiegmund2012} also notes that analysing SQLite only needed about 0.1\% of all possible configurations. This hints to the good scalability of \AFID.

In conclusion \AFID~measures only a quadric amount of configurations to find the performance model of the software system. Depending on which heuristics are chosen, the accuracy and measurement costs can variate. The goal giving explainable predictions was also. \citet{AutomatedFeatureDetectionSiegmund2012} implemented this approach in their tool SPLConquerer\footnote{https://github.com/se-passau/SPLConqueror}. This tool was also used to produce all experiment results.