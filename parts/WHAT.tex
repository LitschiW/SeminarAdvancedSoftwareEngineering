\subsection{WHAT}\label{sec:WHAT}
\WHAT~is a spectral learning approach developed by \citet{FasterDiscoveryofFasterSystemConfigurationsSiegmund2017}. The following section is based on proposing paper of \citet{FasterDiscoveryofFasterSystemConfigurationsSiegmund2017}. \WHAT~aims to find an accurate and stable performance model with fewer samples than the previous methods. To reach this goal it uses spectral and regression tree learning. The idea behind spectral learning is the mathematical concept of \textit{eigenvalues/-vectors} of a distance matrix between configurations. This has the advantage of automatic noise reduction.
 \citet{FasterDiscoveryofFasterSystemConfigurationsSiegmund2017} explain, that when a data set has many irrelevancies or closely associated data parameters $d$, then only a few eigenvectors $e, e\ll d$ are required to characterize the data. To find these important \textit{eigenvalues/-vectors} a simple clustering algorithm is applied to $\mathcal{C}$.
The main advantage of this approach are a reduced sample size and a lower standard deviation compared to previously shown methods \cite{FasterDiscoveryofFasterSystemConfigurationsSiegmund2017}.
\citet{FasterDiscoveryofFasterSystemConfigurationsSiegmund2017} divide \WHAT~into 3 parts.
\\

\noindent
\textbf{1. Spectral Learning}\\
\noindent
This first step of spectral learning is used to cluster all valid configurations. As each configuration is an n-dimensional vector (or n-tuple) it can be placed in an n-dimensional space.\\
\WHAT~gets $N$ different valid configurations as input and is picks a random configuration $N_i$ and two configurations $West$ and  $East$. $West$ is the configuration that is most different to $N_i$ and $East$ is the configuration most different to $West$. In mathematical terms the 'most different' means the largest euclidean distance between two configurations. After that, a straight through $East$ and $West$ is calculated and all configuration are dived into two clusters. This division is based on the median value of all calculated distances. This process is recursively repeated for each sub-cluster until they reach a threshold size. \citet{FasterDiscoveryofFasterSystemConfigurationsSiegmund2017} use $\sqrt{|N|}$ as their termination value. Unlike other clustering algorithms like K-Means, spectral learning runs in linear time of  $\mathcal{O}(2|n|)$.
\\

\noindent
\textbf{2. Spectral Sampling}\\
\noindent
For the actual sampling different strategies can be applied.
The most important is \textit{random sampling}, where one configuration of each leaf cluster is randomly picked as a representative. This representative is then compiled and measured.\\
There are also two other sampling strategies mentioned. \textit{East-West sampling} complies and executes $East$ and $West$ of the current leaf-cluster. Whereas \textit{Exemplar sampling} measures all configurations of a leaf-cluster, but only returns the lowest performance score as a representative. Both of these strategies get outperformed by the \textit{random sampling} strategy.
\\

\noindent
\textbf{3. Regression-Tree Learning}\\
\noindent
In this step a CART is build from the chosen samples. This time the best split is defined as reaching the minimum of $\frac{A}{N}\sigma_1+\frac{B}{N}\sigma_2$$^,$\footnote{The paper (\cite{FasterDiscoveryofFasterSystemConfigurationsSiegmund2017}) defines $A$ and $B$ as sets and $N$ as a (natural) number. So it may be assumed that the formula actually should be $\frac{|A|}{N}\sigma_1+\frac{|B|}{N}\sigma_2$. This does make sense, since this formula weights both standard deviations $\sigma$ proportional to total number of current configurations $N$.}. $\sigma_1$ and $\sigma_2$ are the standard deviations of the corresponding sub-trees. From this CART again decision rules can be derived.

\subsubsection{\textnormal{Results}} of testing \WHAT~on the programs we introduced earlier show that it has an average precision of 93.4\%. Furthermore, the standard deviation is comparably low, which was one of the original targets of \WHAT~\cite{FasterDiscoveryofFasterSystemConfigurationsSiegmund2017}.