\subsection{Variability aware Performance Prediction}

The following section is based upon \cite{VariabilityAwarePerformancePredictionJianmeiSigmundApel}.\\
Variability aware performance prediction is a model based approach to performance predictions. With the help of \textit{Classication and Regression Trees} (short: CART, see \cite{ClassificationandRegressionTrees}) and some configuration samples a statistical model of a program can be created. In thier own tests \citet{VariabilityAwarePerformancePredictionJianmeiSigmundApel} reached an average precision of 94\%.

\def\X{\ensuremath{\mathbf{X}}}
\def\Y{\ensuremath{\mathrm{Y}}}
\def\x{\ensuremath{\mathrm{\mathbf{x}}}}
\def\y{\ensuremath{\mathrm{y}}}

\begin{wrapfigure}{r}{.5\textwidth}
	\vspace{-1\baselineskip}
	\setlength\belowcaptionskip{-\baselineskip}
	\includesvg[width = \linewidth]{figures/VAPPCourse}
	\caption{Overview of the Approach \cite{VariabilityAwarePerformancePredictionJianmeiSigmundApel}}	
	\label{fig:VAPPOverview}
\end{wrapfigure}

\subsubsection[Basic Idea]{\textnormal{The} basic idea} of variability aware performance prediction can be seen in \autoref{fig:VAPPOverview}.
Two cycles can be found. 

The first cycle is outside of the dashed box and describes the basic input-output behaviour of a predictor. A user configures a new configuration $x$ for System $A$ and asks the predictor (dashed box) for a prediction. It replies with a quantitative prediction for $x$'s performance.

In the second cycle a actual prediction is generated based on decision rules which themselves are created by simplifying a performance model. Random samples are used to learn the performance model.\\
Like other approach, the target of variability aware performance prediction is to get accurate predictions with only using a small amount of samples for the creation of the performance model.

\begin{figure}[!]
	\vspace{-1\baselineskip}
	\setlength\belowcaptionskip{-2.5\baselineskip}
	\includesvg[width = \linewidth]{figures/VAPPCARTBinary}
	\caption{Example performance model of X264 generated by CART based on the random sample \cite{VariabilityAwarePerformancePredictionJianmeiSigmundApel}.}	
	\label{fig:VAPPExampleTree}	
\end{figure}
\FloatBarrier %forces the float to appear below the subsubsection
\subsubsection{Statistical Methods}  are used to perform the actual computation. First of all a configuration is defined as an $N$-tuple $(x_1,x_2,x_3,...,x_N)$, where $N$ is the number of all available features. Each $x_i$ represents a feature and can either have the value 1 or 0 depending on whether the feature is selected or not. An actual configuration example would be $\x_j = (x_1=1,x_2=0,x_3=1,\dots,x_N = 1)$. All valid configurations of a system are denoted as $\X$.\\
To each configuration $\x_j$ an actual performance value $y_j$ can be assigned. $\Y$ denotes the performance of all configurations of a system.\\
%It is assumed that all $x_i\in\mathrm{x}_j$ influence the performance of a system. So all of them are considered \textit{predictors}. $y_j$ is the seen as the \textit{response}.
Combining $\Y$ with $\X_S\subset\X$ gives a sample $S$. Now the two problems arise that variability aware performance prediction tries to solve:
\begin{enumerate}
	\item Predict the performance of the not measured configurations $\hat{=}\;\X\backslash\X_S$.
	\item ``Given a sample $S$, the problem is to find a function $f$ that reveals the correlation between $\X_S$ and $\Y_S$ and that makes each configurationâ€™s predicted performance $f(x)$ as close as possible to its actual performance $y$, i.e.:
	\begin{equation}
	f : \X \rightarrow  \mathbb{R} \text{ such that} \sum_{\x,y \in S} L(y,f(\x)) \text{ is minimal}
	\end{equation} 	where $L$ is a loss function to penalize errors in prediction.''\cite{VariabilityAwarePerformancePredictionJianmeiSigmundApel}
\end{enumerate}
This is done with the help of CART. All sample configurations get categorized into a binary trees leafs. A configurations selection of features determines its location in the tree. The distribution of samples inside the tree is determined by CART with the goal of minimizing the total prediction errors per segment (sub-trees). An example tree can be found in \cref{fig:VAPPExampleTree}.
For each leaf one can determine the \textit{local model} $\ell$
\begin{equation}
	\ell_{S_i} = \frac{1}{|S_i|} \sum_{y_j \in S_i} y_j
\end{equation}
As a loss function to penalize the prediction errors \citet{VariabilityAwarePerformancePredictionJianmeiSigmundApel} choose the sum of squared error loss:
\begin{equation}
	\sum_{y_j \in S_i} L(y_i,\ell_{S_i}) = \sum_{y_j \in S_i} (y_j - \ell_{S_i})^2
\end{equation}
Therefore the best split for a segment $S_i$ is found when
\begin{equation*}
\sum_{y_j \in S_{iL}} L(y_i,\ell_{S_{iL}}) + \sum_{y_j \in S_{iR}} L(y_i,\ell_{S_{iR}})
\end{equation*}
is minimal. To prevent \textit{under}- or \textit{overfitting}\cite{ElementsOfStatisticalLearning} the recursive splitting has to be stopped at the right time. This is possible by manual parameter tuning or using a empirical-determined automatic terminator. \\
Now to the actual calculation of the quantitative prediction. Assuming there are $q$ leafs in our tree than $f(\mathrm{x})$ is defined as:
\begin{equation}\textsl{}
f(\mathrm{x})=\sum_{i=1}^{q} \ell_{S_i}I(\mathrm{x}\in S_i)
\end{equation}
where $I(\mathrm{x}\in S_i)$ is an indicator function to indicates that $\mathrm{x}$ belongs to a leaf $S_i$.\\
For the example of \autoref{fig:VAPPExampleTree} $f(\mathrm{x})$ unwraps to:
\begin{align*}
f(x) = 255&* I(x_{14}=1,x_7=0)\\[-0.1cm]
	 + 268&* I(x_{14}=1,x_7=1)\\[-0.1cm]
	 + 402&* I(x_{14}=0,x_{15}=1,x_3=0)\\[-0.1cm]
	 + 508&* I(x_{14}=0,x_{15}=1,x_3=1)\\[-0.1cm]
	 + 571&* I(x_{14}=0,x_{15}=0,x_3=1)\\[-0.1cm]
	 + 626&* I(x_{14}=0,x_{15}=0,x_3=0)
\end{align*}
Every possible configuration $\mathrm{x}$ is associated with a leaf of the tree. Therefore $f(\mathrm{x})$ can always be applied.